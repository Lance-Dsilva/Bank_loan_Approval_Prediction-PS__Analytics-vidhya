# -*- coding: utf-8 -*-
"""Copy of Loan_Prediction_ML_LND.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zCNSg4i8YHl0VgFOy7Kx6CdajkUVrhby

# Importing the libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

"""# Importing the datasets"""

dataset = pd.read_csv("train_ctrUa4K.csv")
dataset2 = pd.read_csv("test_lAUu6dG.csv")
dataset = dataset.drop(['Loan_ID'], axis = 1)
dataset2 = dataset2.drop(['Loan_ID'], axis = 1)

dataset.shape

dataset2.shape

"""# Analysing the Training dataset"""

dataset.head()

dataset.dtypes

dataset.count()

dataset.isna().sum()

dataset["Gender"].isnull().sum()

dataset.info()

dataset["Gender"].value_counts()

dataset["Education"].value_counts()

dataset["Self_Employed"].value_counts()

dataset["Property_Area"].value_counts()

dataset["Loan_Status"].value_counts()

"""# Visualising the datasets"""

categorical_columns = ['Gender', 'Married',
                       'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']
fig,axes = plt.subplots(4,2,figsize=(12,15))
for idx,cat_col in enumerate(categorical_columns):
    row,col = idx//2,idx%2
    sns.countplot(x=cat_col,data=dataset,hue='Loan_Status',ax=axes[row,col])

plt.scatter(dataset['ApplicantIncome'],dataset['CoapplicantIncome'])

import seaborn as sns 
sns.violinplot(dataset['ApplicantIncome'], dataset['Gender']) #Variable Plot
sns.despine()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(dataset['Loan_Status'],dataset['CoapplicantIncome'],color = "yellow")
plt.show()

sns.heatmap(dataset.corr(), annot=True)

fig, ax = plt.subplots()
ax.hist(dataset["Loan_Status"],color = "purple")
ax.set_title('loan approvl counts')
ax.set_xlabel('Loan status')
ax.set_ylabel('Frequency')

"""# Taking Care of Missing Values"""

dataset["Gender"].fillna("Male", inplace = True) 
dataset["Married"].fillna("No", inplace = True) 
dataset["Education"].fillna("Graduate", inplace = True) 
dataset["Self_Employed"].fillna("No", inplace = True) 
dataset["Property_Area"].fillna("Urban", inplace = True)

dataset.isnull().sum()

dataset2["Gender"].fillna("Male", inplace = True) 
dataset2["Married"].fillna("No", inplace = True) 
dataset2["Education"].fillna("Graduate", inplace = True) 
dataset2["Self_Employed"].fillna("No", inplace = True) 
dataset2["Property_Area"].fillna("Urban", inplace = True)

"""# Encodiing the categorical variable"""

train_df_encoded = pd.get_dummies(dataset,drop_first=True)
train_df_encoded.head()

train_df_encoded.shape

test_df_encoded = pd.get_dummies(dataset2,drop_first=True)
test_df_encoded.head()

test_df_encoded.shape

"""# Splitting the dependent and independewnt variable"""

X = train_df_encoded.drop(columns='Loan_Status_Y').values
y = train_df_encoded['Loan_Status_Y'].values

X.shape

X_test_run = test_df_encoded.values

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X[:,0:4] = sc.fit_transform(X[:,0:4])

X_test_run[:,0:4] = sc.fit_transform(X_test_run[:,0:4])

"""# Splitting in to train and test"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)

print(X_train)

"""# Taking Care of numrical missing values"""

from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy='mean')
imp_train = imp.fit(X_train)
X_train = imp_train.transform(X_train)
X_test_imp = imp_train.transform(X_test)

X_test_run[0]

X_test_run= imp_train.transform(X_test_run)

"""# Testing different Clasification Models

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
log_classifier = LogisticRegression()
log_classifier.fit(X_train, y_train)

y_pred = log_classifier.predict(X_test_imp)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average=None)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax, cmap='rainbow'); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Yes', 'No']); ax.yaxis.set_ticklabels(['Yes', 'No']);

"""## Knearest"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test_imp)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average=None)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax, cmap='flag'); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Yes', 'No']); ax.yaxis.set_ticklabels(['Yes', 'No']);

"""## SVM"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test_imp)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average=None)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax, cmap='gist_rainbow'); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Yes', 'No']); ax.yaxis.set_ticklabels(['Yes', 'No']);

"""## Kernal SVM"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test_imp)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average=None)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax,); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Yes', 'No']); ax.yaxis.set_ticklabels(['Yes', 'No']);

"""## Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test_imp)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average=None)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax, cmap='rainbow'); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Yes', 'No']); ax.yaxis.set_ticklabels(['Yes', 'No']);

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test_imp)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average=None)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax,  cmap='flag'); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Yes', 'No']); ax.yaxis.set_ticklabels(['Yes', 'No']);

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test_imp)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average=None)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax, cmap='gist_rainbow'); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Yes', 'No']); ax.yaxis.set_ticklabels(['Yes', 'No']);

"""# Predicting The Test dataset

## Selecting Logistic Regression Based on accuracy_score and f1Score
"""

log_classifier.predict(X_test_run)